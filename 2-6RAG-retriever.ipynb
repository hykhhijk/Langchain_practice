{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store Retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data -> Text split\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "loader = PyMuPDFLoader('pdf.pdf')\n",
    "data = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    encoding_name='cl100k_base'\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(data)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터스토어에 문서 임베딩을 저장\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name='jhgan/ko-sbert-nli',\n",
    "    model_kwargs={'device':'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings':True},\n",
    ")\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents,\n",
    "                                   embedding = embeddings_model,\n",
    "                                   distance_strategy = DistanceStrategy.COSINE  \n",
    "                                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'pdf.pdf', 'file_path': 'pdf.pdf', 'page': 12, 'total_pages': 19, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20241113010901Z', 'modDate': \"D:20241204093544+09'00'\", 'trapped': ''}, page_content='Figure 8: Correctness achieved by prompting LLaMA with various numbers of documents retrieved\\nwith BGE-base and ColBERT, k, included in the prompts. Optimal performance is observed with\\nk = 4 or 5 for ASQA and NQ, while optimal performance for QAMPARI is achieved with k = 10.\\nTable 8: Correctness and citation quality on ASQA achieved with LLaMA with various numbers of\\nColBERT retrieved documents, k, included in the prompt.\\nRet.\\nEM Recall\\nCitation Recall\\nCitation Precision\\nk\\nRec@k\\nMean\\n95% CI\\nMean\\n95% CI\\nMean\\n95% CI\\ngold\\n1\\n47.466\\n45.304 - 49.426\\n46.326\\n44.123 - 48.524\\n46.294\\n44.082 - 48.686\\n0\\n0\\n23.327\\n21.355 - 25.252\\n-\\n-\\n-\\n-\\n1\\n0.098\\n31.928\\n29.979 - 33.962\\n34.944\\n32.569 - 37.116\\n43.637\\n41.007 - 46.503\\n2\\n0.179\\n36.262\\n34.089 - 38.509\\n52.282\\n49.951 - 54.57\\n58.651\\n56.199 - 60.848\\n3\\n0.242\\n36.548\\n34.33 - 38.787\\n54.545\\n52.246 - 56.676\\n57.067\\n54.78 - 59.45\\n4\\n0.291\\n36.813\\n34.688 - 38.9\\n50.63\\n48.394 - 52.85\\n51.505\\n49.236 - 53.808\\n5\\n0.328\\n36.712\\n34.668 - 38.79\\n46.293\\n43.897 - 48.806\\n44.565\\n42.292 - 46.945\\n10\\n0.447\\n35.016\\n32.844 - 37.057\\n37.334\\n34.937 - 39.771\\n32.033\\n29.975 - 34.319\\n13')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검색 쿼리\n",
    "query = '카카오뱅크의 환경목표와 세부추진내용을 알려줘'\n",
    "\n",
    "# 가장 유사도가 높은 문장을 하나만 추출\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 1})\n",
    "\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "print(len(docs))\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'pdf.pdf', 'file_path': 'pdf.pdf', 'page': 12, 'total_pages': 19, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20241113010901Z', 'modDate': \"D:20241204093544+09'00'\", 'trapped': ''}, page_content='Figure 8: Correctness achieved by prompting LLaMA with various numbers of documents retrieved\\nwith BGE-base and ColBERT, k, included in the prompts. Optimal performance is observed with\\nk = 4 or 5 for ASQA and NQ, while optimal performance for QAMPARI is achieved with k = 10.\\nTable 8: Correctness and citation quality on ASQA achieved with LLaMA with various numbers of\\nColBERT retrieved documents, k, included in the prompt.\\nRet.\\nEM Recall\\nCitation Recall\\nCitation Precision\\nk\\nRec@k\\nMean\\n95% CI\\nMean\\n95% CI\\nMean\\n95% CI\\ngold\\n1\\n47.466\\n45.304 - 49.426\\n46.326\\n44.123 - 48.524\\n46.294\\n44.082 - 48.686\\n0\\n0\\n23.327\\n21.355 - 25.252\\n-\\n-\\n-\\n-\\n1\\n0.098\\n31.928\\n29.979 - 33.962\\n34.944\\n32.569 - 37.116\\n43.637\\n41.007 - 46.503\\n2\\n0.179\\n36.262\\n34.089 - 38.509\\n52.282\\n49.951 - 54.57\\n58.651\\n56.199 - 60.848\\n3\\n0.242\\n36.548\\n34.33 - 38.787\\n54.545\\n52.246 - 56.676\\n57.067\\n54.78 - 59.45\\n4\\n0.291\\n36.813\\n34.688 - 38.9\\n50.63\\n48.394 - 52.85\\n51.505\\n49.236 - 53.808\\n5\\n0.328\\n36.712\\n34.668 - 38.79\\n46.293\\n43.897 - 48.806\\n44.565\\n42.292 - 46.945\\n10\\n0.447\\n35.016\\n32.844 - 37.057\\n37.334\\n34.937 - 39.771\\n32.033\\n29.975 - 34.319\\n13')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MMR - 다양성 고려 (lambda_mult = 0.5)\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type='mmr',\n",
    "    search_kwargs={'k': 5, 'fetch_k': 50}\n",
    ")\n",
    "\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "print(len(docs))\n",
    "docs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'pdf.pdf', 'file_path': 'pdf.pdf', 'page': 12, 'total_pages': 19, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20241113010901Z', 'modDate': \"D:20241204093544+09'00'\", 'trapped': ''}, page_content='Figure 8: Correctness achieved by prompting LLaMA with various numbers of documents retrieved\\nwith BGE-base and ColBERT, k, included in the prompts. Optimal performance is observed with\\nk = 4 or 5 for ASQA and NQ, while optimal performance for QAMPARI is achieved with k = 10.\\nTable 8: Correctness and citation quality on ASQA achieved with LLaMA with various numbers of\\nColBERT retrieved documents, k, included in the prompt.\\nRet.\\nEM Recall\\nCitation Recall\\nCitation Precision\\nk\\nRec@k\\nMean\\n95% CI\\nMean\\n95% CI\\nMean\\n95% CI\\ngold\\n1\\n47.466\\n45.304 - 49.426\\n46.326\\n44.123 - 48.524\\n46.294\\n44.082 - 48.686\\n0\\n0\\n23.327\\n21.355 - 25.252\\n-\\n-\\n-\\n-\\n1\\n0.098\\n31.928\\n29.979 - 33.962\\n34.944\\n32.569 - 37.116\\n43.637\\n41.007 - 46.503\\n2\\n0.179\\n36.262\\n34.089 - 38.509\\n52.282\\n49.951 - 54.57\\n58.651\\n56.199 - 60.848\\n3\\n0.242\\n36.548\\n34.33 - 38.787\\n54.545\\n52.246 - 56.676\\n57.067\\n54.78 - 59.45\\n4\\n0.291\\n36.813\\n34.688 - 38.9\\n50.63\\n48.394 - 52.85\\n51.505\\n49.236 - 53.808\\n5\\n0.328\\n36.712\\n34.668 - 38.79\\n46.293\\n43.897 - 48.806\\n44.565\\n42.292 - 46.945\\n10\\n0.447\\n35.016\\n32.844 - 37.057\\n37.334\\n34.937 - 39.771\\n32.033\\n29.975 - 34.319\\n13')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서 객체의 metadata를 이용한 필터링\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={'filter': {'format':'PDF 1.6'}}\n",
    ")\n",
    "\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "print(len(docs))\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter가 exhautive search 일지 궁금함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'제공된 문맥에는 카카오뱅크의 환경목표와 세부 추진 내용에 대한 정보가 포함되어 있지 않습니다. 추가적인 정보가 필요합니다.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# Retrieval\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type='mmr',\n",
    "    search_kwargs={'k': 5, 'lambda_mult': 0.15}\n",
    ")\n",
    "\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Prompt\n",
    "template = '''Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Model\n",
    "llm = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join([d.page_content for d in docs])\n",
    "\n",
    "# Chain\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "response = chain.invoke({'context': (format_docs(docs)), 'question':query})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "솔직히 이게 될지는 몰랐다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Query Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What is the ESG strategy of SK?', \"2. Can you explain SK's ESG strategy?\", '3. How does SK approach ESG in its business strategy?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 멀티 쿼리 생성\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "question = 'SK의 ESG 전략은 무엇인가요?'\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    temperature=0,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=llm\n",
    ")\n",
    "\n",
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)\n",
    "\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'pdf.pdf', 'file_path': 'pdf.pdf', 'page': 13, 'total_pages': 19, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20241113010901Z', 'modDate': \"D:20241204093544+09'00'\", 'trapped': ''}, page_content='Table 9: Correctness and citation quality on NQ achieved with Mistral with various numbers of\\nBGE-base retrieved documents, k, included in the prompt.\\nRet.\\nEM Recall\\nCitation Recall\\nCitation Precision\\nk\\nRec@k\\nMean\\n95% CI\\nMean\\n95% CI\\nMean\\n95% CI\\ngold\\n1\\n84.646\\n83.222 - 86.042\\n74.66\\n73.349 - 76.006\\n64.407\\n63.179 - 65.615\\n0\\n46.696\\n44.836 - 48.467\\n-\\n-\\n-\\n-\\n1\\n0.072\\n37.871\\n36.164 - 39.726\\n31.101\\n29.567 - 32.776\\n36.4\\n34.695 - 38.181\\n2\\n0.117\\n40.925\\n39.02 - 42.792\\n43.51\\n41.923 - 45.18\\n39.554\\n38.067 - 41.129\\n3\\n0.152\\n45.301\\n43.497 - 47.127\\n47.423\\n45.796 - 48.944\\n40.574\\n39.101 - 42.005\\n4\\n0.181\\n49.735\\n47.902 - 51.639\\n49.943\\n48.38 - 51.534\\n43.404\\n42.007 - 44.826\\n5\\n0.205\\n51.421\\n49.559 - 53.226\\n51.062\\n49.51 - 52.51\\n43.69\\n42.326 - 45.13\\n10\\n0.279\\n56.778\\n54.916 - 58.654\\n55.869\\n54.395 - 57.241\\n46.267\\n45.014 - 47.542\\n20\\n0.355\\n61.145\\n59.391 - 62.883\\n57.173\\n55.754 - 58.595\\n46.497\\n45.282 - 47.711\\n100\\n0.53\\n60.333\\n58.548 - 62.143\\n49.816\\n48.332 - 51.397\\n36.671\\n35.502 - 37.921\\nTable 10: Correctness and citation quality on NQ achieved with Mistral with various numbers of\\nColBERT retrieved documents, k, included in the prompt.\\nRet.\\nEM Recall\\nCitation Recall\\nCitation Precision\\nk\\nRec@k\\nMean\\n95% CI\\nMean\\n95% CI\\nMean\\n95% CI\\ngold\\n1\\n84.646\\n83.222 - 86.042\\n74.66\\n73.349 - 76.006\\n64.407\\n63.179 - 65.615\\n0\\n46.696\\n44.836 - 48.467\\n-\\n-\\n-\\n-\\n1\\n0.122\\n49.567\\n47.797 - 51.463\\n67.754\\n66.276 - 69.245\\n77.295\\n75.828 - 78.863\\n2\\n0.179\\n55.398\\n53.542 - 57.244\\n72.288\\n70.925 - 73.617\\n68.703\\n67.417 - 69.971\\n3\\n0.214\\n58.486\\n56.714 - 60.204\\n71.02\\n69.784 - 72.254\\n63.799\\n62.58 - 65.048\\n4\\n0.237\\n60.41\\n58.688 - 62.178\\n69.883\\n68.64 - 71.088\\n60.987\\n59.797 - 62.236\\n5\\n0.254\\n61.916\\n60.134 - 63.66\\n68.635\\n67.291 - 69.896\\n59.862\\n58.683 - 61.101\\n10\\n0.321\\n64.171\\n62.424 - 66.056\\n67.233\\n65.901 - 68.508\\n55.348\\n54.1 - 56.477\\n20\\n0.381\\n62.558\\n60.874 - 64.434\\n64.701\\n63.314 - 66.002\\n50.239\\n49.051 - 51.368\\n100\\n0.506\\n58.384\\n56.502 - 60.204\\n50.21\\n48.575 - 51.79\\n35.233\\n34.014 - 36.53')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['SK의 ESG 전략에 대해 설명해 주실 수 있나요?  ', 'SK 그룹이 채택하고 있는 ESG 접근 방식은 어떤 것들이 있나요?  ', 'SK의 환경, 사회, 지배구조(ESG) 관련 정책과 목표는 무엇인가요?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'SK의 ESG 전략은 무엇인가요?'\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=llm\n",
    ")\n",
    "\n",
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)\n",
    "\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "api 모델의 다국어 처리능력에 따른 query 변환도 해주는 듯? 못하는게 없는것 같어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# 기본 검색기\n",
    "\n",
    "question = 'how to search optimal k for retriever?'\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "base_retriever = vectorstore.as_retriever(\n",
    "                                search_type='mmr',\n",
    "                                search_kwargs={'k':7, 'fetch_k': 20})\n",
    "\n",
    "docs = base_retriever.get_relevant_documents(question)\n",
    "print(len(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# 문서 압축기를 연결하여 구성\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=base_retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "print(len(compressed_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'pdf.pdf', 'file_path': 'pdf.pdf', 'page': 3, 'total_pages': 19, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20241113010901Z', 'modDate': \"D:20241204093544+09'00'\", 'trapped': ''}, page_content='We first analyze how many retrieved documents should be included in the LLM context window to maximize correctness on the selected QA tasks. This is shown as a function of the number of retrieved nearest neighbors, k. Incorporating the retrieved documents narrows the performance disparity between the closed-book scenario (k=0) and the gold-document-only ceiling. However, the performance of the evaluated retrieval systems still significantly lags behind the ideal. ColBERT usually outperforms BGE by a small margin. Optimal performance is observed with k = 10 or 20. Correctness on QA begins to plateau around 5-10 documents. We find that Mistral performs best for all three datasets with 10 or 20 documents. LLaMA performs best when k = 4 or 5 for ASQA and NQ, but k = 10 for QAMPARI. We also find that adding the citation prompt to NQ results in almost no change to performance until k > 10. Given these results, we conducted all subsequent analyses and experiments with 5-10 context documents, as these were generally good settings for QA performance even if some of the gold documents are missed. Based on the results above, we hypothesized that the ideal number of documents to include in a RAG pipeline is directly related to the number of gold documents that are retrieved within that k.'),\n",
       " Document(metadata={'source': 'pdf.pdf', 'file_path': 'pdf.pdf', 'page': 0, 'total_pages': 19, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20241113010901Z', 'modDate': \"D:20241204093544+09'00'\", 'trapped': ''}, page_content='We aim to address questions that will enable practitioners to design retrieval systems tailored for use in RAG pipelines. For example, what are the weaknesses of the typical search and retrieval setup in RAG systems? Which search hyperparameters matter for RAG task performance?'),\n",
       " Document(metadata={'source': 'pdf.pdf', 'file_path': 'pdf.pdf', 'page': 4, 'total_pages': 19, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20241113010901Z', 'modDate': \"D:20241204093544+09'00'\", 'trapped': ''}, page_content='Optimal QA correctness is achieved at k = 10, while it is k = 5 for citation recall.')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llm prompt를 통한 generation 느낌의 압축은 아니고 MRC의 position 위치를 정해주는 압축 같음, 실제 PDF와 완전히 matching 하는 내용들임"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
